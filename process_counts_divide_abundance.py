import pickle
from collections import ChainMap #hopefully this works, if not we can merge with comprehension
import pandas as pd
import anndata
import numpy as np
import scanpy as sc

#Section 0: Load in Abundance stuff for later
#Get all the entries written in the pkl files. Note that these files will be created in the directory from which you run CoPTR. They are created by SEAN's forked version of CoPTR. In the future, we might want to do this using a database instead of reading and writing from pkl files, but I did this in a stupid simple way.

data = []
with open('shared_dict_ref.pkl', 'rb') as fr:
    try:
        while True:
            data.append(pickle.load(fr))
    except EOFError:
        pass

with open('shared_dict_contig.pkl', 'rb') as fr: #forgot to make this .pkl, oops
    try:
        while True:
            data.append(pickle.load(fr))
    except EOFError:
        pass

#Combine all the indivudal written entries into big dictionary
total_dict = dict(ChainMap(*data))

#Section 1: PTR stuff
#Load in PTR csv generated by CoPTR. It has a good header (sample names), but the first column should just be row attributes instead of a real collumn
df = pd.read_csv('out.csv')

#Replace the na values in the df with 0s.
df.fillna(0,inplace=True)

#change the row index values to the first column
df.index = [i for i in df.iloc[:,0]]

#remove that first column and put the df into anndata
df = df.iloc[:,1:]
adata_PTR = anndata.AnnData(df)

#Now we transpose it (already a AnnData object)
adata_PTR_trans = adata_PTR.transpose()

#Section 2: Loading Abundances
#NOW WE HAVE VELOCITY MATRIX. We want new matrix of same dimensions and attributes for the abundance

#Get the obs and var atrributes from it
obs_len = adata_PTR_trans.n_obs 
var_len = adata_PTR_trans.n_vars
obs = adata_PTR_trans.obs_names
var = adata_PTR_trans.var_names

#Create a new zeros df for abundances
abund_df = pd.DataFrame(np.zeros((obs_len, var_len)))
abund_df.index = list(obs)
abund_df.columns = list(var)

#sort df for faster access
abund_df.sort_index(inplace=True)

#Edit this new dataframe with abundances
#May want to look into optimizations, the way I'm doing this may not scale well
for tup, count in total_dict.items():
    abund_df.loc[tup[1],tup[0]] = count

#Section 3: Normalize abundance from genome length
#this assumes that there's a stats.tsv file in this directory obtained by running the get_lengths_from_fasta.sh

genome_lengths = pd.read_csv("stats.tsv", sep="\t")

#only one relavent columns, the file name (which is species) and total length
genome_lengths = genome_lengths[["file", "sum_len"]]

#sort by species so this is the same order
name_to_index = {name:index for index, name in enumerate(list(var))}
genome_lengths['index'] = genome_lengths['file'].map(name_to_index) #makes new column to sort by
genome_lengths.sort_values(by = 'index', inplace = True)
genome_lengths.drop(columns=['index', 'file'], inplace = True)

#now we have a dataframe thats just one column (sum_len)
len_series = genome_lengths['sum_len'].values

#make it so that we're dividing instead of multiplying
len_series = len_series / 1000000
len_series = np.reciprocal(len_series)

#apply this vector to the abundance dataframe to normalize based on length
abund_df = abund_df * len_series

#add 1, to handle log(0) issue
#Take the log of the abundances
abund_df += 1
abund_df = np.log(abund_df)

#Section 4: Combine into AnnData
#convert this df to a adata object and make it the PTR object a layer of it
adata_abund = anndata.AnnData(abund_df)
#Need to refer to the abundance as a layer, so make it an alias for the base layer X. There may be a better way to do this, not experienced with adata
adata_abund.layers["abundance"] = adata_abund.X
adata_abund.layers["PTR"] = adata_PTR_trans.X

#Check
print(adata_abund.to_df(layer="abundance"))
print(adata_abund.to_df(layer="PTR"))

#Section 5: cellrank stuff
#TEST CELLRANK USAGE

#ADD CLUSTER LABELS
ct = np.random.choice(["Healthy","Sick"], size=(adata_abund.n_obs,))
adata_abund.obs["clusters"] = pd.Categorical(ct)  # Categoricals are preferred for efficiency

print(adata_abund.obs["clusters"])

#TEST FILTRATION AND NORMALIZATION
#sc.pp.filter_genes(adata_abund, min_cells = 5) #do filtration once you have a larger dataset, can tweak parameters to decide species to exclude based on # counts
sc.pp.normalize_total(adata_abund, layer ="abundance") #normalization does this for the base layer X as well

print(adata_abund.to_df(layer="abundance"))
print(adata_abund.to_df(layer="PTR"))

sc.tl.pca(adata_abund, random_state=0)
sc.pp.neighbors(adata_abund, random_state=0)

#Getting VELOCITY Kernel
from cellrank.kernels import VelocityKernel
vk = VelocityKernel(adata_abund, gene_subset=adata_abund.var, vkey="PTR",xkey="abundance")
vk.compute_transition_matrix()

#Getting CONNECTIVITY Kernel
from cellrank.kernels import ConnectivityKernel
ck = ConnectivityKernel(adata_abund).compute_transition_matrix()

#Combined kernel (linear combination of matrices)
kernel = 0.8*vk + 0.2*ck

#ESTIMATOR
#Can't import GPCCA from cellrank.tl.estimators, cellrank.tl has been depreciated. Import this way:
import cellrank as cr
g = cr.estimators.GPCCA(kernel)

#Compute your schur decomposition. cellrank uses 20 components in the example. This is the part where you'd want to use KRYLOV instead for very large sample counts. Using 5 for baby test data
g.compute_schur(n_components=5)

g.plot_spectrum(real_only=True)

#compute your macrostates, using the clusters annotation in adata_abund obs.
g.compute_macrostates(n_states=2, n_cells=2, cluster_key="clusters")
